# DGCL - Distance-Guided Contrastive Learning

## Resumo
O aprendizado autossupervisionado (Self-Supervised Learning — SSL) tem se consolidado como um paradigma promissor no campo da visão computacional, por permitir o aprendizado de representações visuais robustas sem a necessidade de grandes volumes de dados rotulados. Entre suas vertentes, o aprendizado contrastivo destaca-se por induzir a formação de representações discriminativas por meio da aproximação de pares positivos e distanciamento de pares negativos. Entretanto, a seleção aleatória desses pares limita a exploração eficiente da estrutura geométrica subjacente aos dados, resultando em representações com separabilidade subótima entre classes. Com o objetivo de mitigar essa limitação, este trabalho propõe o Aprendizado Contrastivo Guiado por Distância (Distance-Guided Contrastive Learning — DGCL), uma abordagem autossupervisionada que introduz um mecanismo de seleção de pares baseado na estrutura geométrica do espaço latente. A proposta consiste em identificar, para cada amostra âncora, os exemplos mais informativos, as amostras positivas mais distantes dentro da mesma classe (hard positives) e as amostras negativas mais próximas de classes diferentes (hard negatives), utilizando projeções obtidas via t-Distributed Stochastic Neighbor Embedding (t-SNE). Essa política de seleção orientada pela distância é aplicada iterativamente, refinando progressivamente o espaço de representações. Os experimentos realizados nos conjuntos de dados CIFAR-10, FER-13, KDEF e RAF-DB demonstram ganhos expressivos de desempenho em relação a modelos sem aprendizado contrastivo. Observa-se, em especial, que o DGCL promove uma compactação intra-classe mais pronunciada e uma separação inter-classe mais consistente, resultando em representações latentes mais coerentes e discriminativas. As análises qualitativas baseadas em projeções t-SNE e as matrizes de confusão confirmam a capacidade do método em estruturar o espaço latente de maneira semanticamente significativa.
